{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd943e8",
   "metadata": {},
   "source": [
    "# Language Identification Hackathon\n",
    "\n",
    "©  Explore Data Science Academy\n",
    "\n",
    "---\n",
    "\n",
    "### Honour Code\n",
    "\n",
    "I ******, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "<img src=\"climate_change.jpg\" width=\"800px\">\n",
    "    <figcaption><p text_align = \"center\">\n",
    "    \n",
    "    \n",
    "## Language identification\n",
    "\n",
    "Overview\n",
    "South Africa is a multicultural society that is characterised by its rich linguistic diversity. Language is an indispensable tool that can be used to deepen democracy and also contribute to the social, cultural, intellectual, economic and political life of the South African society.\n",
    "\n",
    "The country is multilingual with 11 official languages, each of which is guaranteed equal status. Most South Africans are multilingual and able to speak at least two or more of the official languages.\n",
    "From South African Government"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a498c01",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c0024",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section the required packages are imported, and briefly discuss, the libraries that will be used throughout the analysis and modelling. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d1942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Libraries for importing and loading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Libraries for data preparation \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# Libraries for data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Building classification models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Libraries for assessing model accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af804878",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading Data\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ab649",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_set.csv')\n",
    "test_df = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946637a4",
   "metadata": {},
   "source": [
    "Let's get the number of rows and columns of the train and test datasets,also lets have a preview of the first few rows of tha datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4cd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print( test_df.shape)\n",
    "\n",
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37489b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = list(train_df.lang_id.unique())\n",
    "print(type_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41d753",
   "metadata": {},
   "source": [
    "###### Create a copy\n",
    "\n",
    "The first step in the preprocessing is to create a copy of the train dataframe for the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of label classes\n",
    "fig,ax = plt.subplots()\n",
    "train_df['lang_id'].value_counts().plot(kind = 'bar', facecolor='g', alpha=0.65)\n",
    "ax.set_xlabel('lang_id')\n",
    "ax.set_ylabel('lang_id count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849ea60",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c205e92",
   "metadata": {},
   "source": [
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\" (see the [Stanford Tokeniser](https://nlp.stanford.edu/software/tokenizer.html)). We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28439f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(train_df['text'].tolist())[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f34708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    \n",
    "    # replace the html characters with \" \"\n",
    "    tweet=re.sub('<.*?>', ' ', tweet)\n",
    "    \n",
    "    #Removal of numbers\n",
    "    tweet= re.sub(r'\\d+', ' ', tweet)\n",
    "\n",
    "    \n",
    "    #convert the tokens back to a string\n",
    "    tweet =' '.join(tweet)\n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(tweet_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b186b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(train_df['text'].tolist())[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704960b",
   "metadata": {},
   "source": [
    "The use of preprocessed data in Word Cloud makes it easy to identify the relevant words as opposed to many instances of https and other types of noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420104fb",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa4c56",
   "metadata": {},
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c7d89",
   "metadata": {},
   "source": [
    "For modeling, we will convert the dataframes to a vector form for feature extraction.\n",
    "\n",
    "I will be using the TFIDF vectorizer. This vectorizer is better than bag of words or countvectorizer because it makes use of both the word frequency and importance. Countvectorizer makes use of only the word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "# separate into x and y\n",
    "# Seperate features and tagret variables\n",
    "X = train_df['text']\n",
    "y = train_df['lang_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58a5ff",
   "metadata": {},
   "source": [
    "## Building classification models\n",
    "\n",
    "We will be making use of a pipeline to build our classification models. This pipeline will vectorize the text data before fitting it to our chosen model.\n",
    "\n",
    "The following 5 models will be considered:\n",
    "\n",
    "- Random forest\n",
    "- Naive Bayes\n",
    "- K nearest neighbors\n",
    "- Logistic regression\n",
    "- Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd5517",
   "metadata": {},
   "source": [
    "#### Train - Validation split\n",
    "\n",
    "Before we pass our data through our custom pipelines we have to split our train data into features and target variables. After this step we can split our train data into a train and validation set. This will allow us to evaluate our model performance and chose the best model to use for our submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train & validation (20%) for model training\n",
    "\n",
    "\n",
    "\n",
    "# Split the train data to create validation dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350f6ef",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "\n",
    "Pipelines consist of 2 steps, vectorization and model fitting.\n",
    "\n",
    "Machines, unlike humans, cannot understand the raw text. Machines can only see numbers. Particularly, statistical techniques such as machine learning can only deal with numbers. Therefore, we need to convert our text into numbers.\n",
    "\n",
    "The TFIDF vectorizer assigns word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Another advantage of this method is that the resulting vectors are already scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41749916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = Pipeline([('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "               \n",
    "               ('clf', RandomForestClassifier(max_depth=5,n_estimators=100))\n",
    "              ])\n",
    "\n",
    "\n",
    "# K-NN Classifier\n",
    "knn = Pipeline([('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "                ('clf', KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2))\n",
    "               ])\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "lr = Pipeline([('tfidf',TfidfVectorizer(max_features=5000)),\n",
    "               ('clf',LogisticRegression(C=1,class_weight='balanced',max_iter=1000))\n",
    "              ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8eb23",
   "metadata": {},
   "source": [
    "#### Train the models\n",
    "\n",
    "The models are trained by passing the train data through each custom pipeline. The trained models are then used to predict the classes for the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18efc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and predict Random forest \n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_valid)\n",
    "\n",
    "#train and predict K - nearest neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_valid)\n",
    "\n",
    "#train and predict Linear regression\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd90fd0",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The performance of each model will be evaluated based on the precision, accuracy and F1 score achieved when the model is used to predict the classes for the validation data. We will be looking at the following to determine and visualize these metrics:\n",
    "\n",
    "Classification report\n",
    "Confusion matrix\n",
    "The best model will be selected based on the weighted F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the random forest model\n",
    "print(metrics.classification_report(y_valid, y_pred_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the K-nearest neighbors model\n",
    "print(metrics.classification_report(y_valid, y_pred_knn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the model\n",
    "print(metrics.classification_report(y_valid, y_pred_lr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd157e",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Linear SVC has achieved the highest F1 score of 0.70 and is therefore our model of choice moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b08bc",
   "metadata": {},
   "source": [
    "### Cleaning  Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1539ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df ['text'] = test_df ['text'].apply(tweet_cleaner)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7283d",
   "metadata": {},
   "source": [
    "### generate the csv file to submmit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_kaggle_csv(model, df):\n",
    "    \n",
    "    #load the test data to a varable \"X_unseen\"\n",
    "    X_test = df['text']\n",
    "    \n",
    "    #Make a prediction on the test data with the trained model\n",
    "    mypreds = model.predict(X_test)\n",
    "    \n",
    "    #Reset the index of the test data\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    #Make a copy of the tweet id \n",
    "    index = df['index']\n",
    "    \n",
    "    #Convert the tweet_id and the prediction \n",
    "    kaggle = pd.DataFrame({'index' : index, \n",
    "                                  'lang_id': mypreds})\n",
    "    \n",
    "    #convert file to csv\n",
    "    kaggle.to_csv('kaggle.csv', index=False)\n",
    "\n",
    "    return kaggle\n",
    "gen_kaggle_csv(rf, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48824d",
   "metadata": {},
   "source": [
    "### Pickle Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec91bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle_file(model, file_name):\n",
    "    # import the pickle module\n",
    "    import pickle\n",
    "    \n",
    "    #asign a path to the file_name \n",
    "    model_save_path = file_name \n",
    "    \n",
    "    #save file to thespecified path\n",
    "    with open(model_save_path,'wb') as file: \n",
    "        pickle.dump(model,file)\n",
    "    \n",
    "    return  model_save_path\n",
    "\n",
    "save_pickle_file(lsvc, \"lsvc_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d127c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9acbeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546978d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83c4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
